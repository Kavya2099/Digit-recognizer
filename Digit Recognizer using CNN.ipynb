{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"},{"sourceId":8335126,"sourceType":"datasetVersion","datasetId":4950013}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kavya2099/digit-recognizer?scriptVersionId=175991551\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-06T10:24:23.46686Z","iopub.execute_input":"2024-05-06T10:24:23.467324Z","iopub.status.idle":"2024-05-06T10:24:23.931794Z","shell.execute_reply.started":"2024-05-06T10:24:23.467288Z","shell.execute_reply":"2024-05-06T10:24:23.930507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reading training and testing dataset","metadata":{}},{"cell_type":"code","source":"train= pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntrain.shape\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:23.933822Z","iopub.execute_input":"2024-05-06T10:24:23.934407Z","iopub.status.idle":"2024-05-06T10:24:27.486223Z","shell.execute_reply.started":"2024-05-06T10:24:23.934365Z","shell.execute_reply":"2024-05-06T10:24:27.484971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test= pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\ntest.shape\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:27.4878Z","iopub.execute_input":"2024-05-06T10:24:27.488139Z","iopub.status.idle":"2024-05-06T10:24:29.461603Z","shell.execute_reply.started":"2024-05-06T10:24:27.488105Z","shell.execute_reply":"2024-05-06T10:24:29.460506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"dropping label column from x_train dataset and adding it in y_train ","metadata":{}},{"cell_type":"code","source":"y_train= train['label']\ny_train","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:29.464083Z","iopub.execute_input":"2024-05-06T10:24:29.46444Z","iopub.status.idle":"2024-05-06T10:24:29.473498Z","shell.execute_reply.started":"2024-05-06T10:24:29.46441Z","shell.execute_reply":"2024-05-06T10:24:29.472303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train= train.drop(labels='label',axis=1)\nx_train","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:29.474682Z","iopub.execute_input":"2024-05-06T10:24:29.474969Z","iopub.status.idle":"2024-05-06T10:24:29.644231Z","shell.execute_reply.started":"2024-05-06T10:24:29.474945Z","shell.execute_reply":"2024-05-06T10:24:29.642608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualizing the count of numbers","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6,6))\ncount= sns.countplot(x=y_train)\ny_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:29.646431Z","iopub.execute_input":"2024-05-06T10:24:29.647137Z","iopub.status.idle":"2024-05-06T10:24:30.682612Z","shell.execute_reply.started":"2024-05-06T10:24:29.647069Z","shell.execute_reply":"2024-05-06T10:24:30.681342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To see a sample image","metadata":{}},{"cell_type":"code","source":"image=x_train.iloc[0]\n# Convert the dataset to a 2D numpy array\nimage_array = image.to_numpy()\n\n# Reshape the 1D array to a 2D image (e.g., 5x5 pixels)\nimage_shape = (28, 28) #this is based on 784 values which is from 28x28\nimage = image_array.reshape(image_shape)\n\n# Display the image\nplt.imshow(image, cmap='gray') \n#The cmap parameter specifies the color map to be used for displaying the image. In this case, 'gray' indicates that the image should be displayed in grayscale (black and white).\nplt.axis('off')  # Hide axes\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:30.683898Z","iopub.execute_input":"2024-05-06T10:24:30.684274Z","iopub.status.idle":"2024-05-06T10:24:30.763678Z","shell.execute_reply.started":"2024-05-06T10:24:30.684227Z","shell.execute_reply":"2024-05-06T10:24:30.762519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image=x_train.iloc[78]\n# Convert the dataset to a 2D numpy array\nimage_array = image.to_numpy()\n\n# Reshape the 1D array to a 2D image (e.g., 5x5 pixels)\nimage_shape = (28, 28) #this is based on 784 values which is from 28x28\nimage = image_array.reshape(image_shape)\n\n# Display the image\nplt.imshow(image, cmap='gray') \n#The cmap parameter specifies the color map to be used for displaying the image. In this case, 'gray' indicates that the image should be displayed in grayscale (black and white).\nplt.axis('off')  # Hide axes\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:30.765216Z","iopub.execute_input":"2024-05-06T10:24:30.766072Z","iopub.status.idle":"2024-05-06T10:24:30.836345Z","shell.execute_reply.started":"2024-05-06T10:24:30.765972Z","shell.execute_reply":"2024-05-06T10:24:30.835208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalization, Reshape and Label Encoding\n## Normalization:\n**Purpose**: Normalization ensures that pixel values are within a consistent range, typically between 0 and 1 or -1 and 1.\n\n**Reasons**:\n* **Stabilizes Training**: Normalizing pixel values prevents large variations in input data, which can lead to unstable training. It helps the model converge faster and more reliably.\n* **Gradient Descent**: Gradient descent optimization algorithms work better when features are on a similar scale.\n* **Activation Functions**: Some activation functions (e.g., sigmoid, tanh) perform better with normalized inputs.\n* **Example**: Divide pixel values by 255 to scale them between 0 and 1.\n\n## Reshaping:\n**Purpose**: Images are typically represented as 2D arrays (height x width x channels). CNNs expect a specific input shape.\n\n**Reasons**:\n* **Input Shape**: CNNs require a consistent input shape (e.g., (height, width, channels)).\n* **Flattening**: If using fully connected layers after CNN layers, reshaping is necessary to flatten the 2D feature maps into a 1D vector.\n* **Example**: Reshape a grayscale image (28x28 pixels) to (28, 28, 1) or an RGB image (28x28x3) to (28, 28, 3).\n\n## Label Encoding:\n**Purpose**: Convert categorical labels (classes) into numerical representations.\n\n**Reasons**:\n* **Model Input**: CNNs require numerical labels for training.\n* **Loss Calculation**: Loss functions (e.g., cross-entropy) compare predicted class probabilities with true labels.\n* **Example**: Encode class labels (e.g., “cat,” “dog,” “bird”) as integers (e.g., 0, 1, 2).\n\n2 => [0,0,1,0,0,0,0,0,0,0]\n\n4 => [0,0,0,0,1,0,0,0,0,0]\n\n### Can We Proceed Without These Steps?\n\n* Technically, you can train a CNN without normalization, reshaping, or label encoding, but it’s not recommended.\n* Skipping normalization may lead to slow convergence or poor performance.\n* Skipping reshaping will cause shape mismatches between input data and model architecture.\n* Skipping label encoding will result in incorrect loss calculations and misinterpretation of class probabilities.\n\n","metadata":{}},{"cell_type":"code","source":"#normalize\n\nx_train=x_train/255.0 \ntest=test/255.0 ","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:30.837641Z","iopub.execute_input":"2024-05-06T10:24:30.838276Z","iopub.status.idle":"2024-05-06T10:24:30.990876Z","shell.execute_reply.started":"2024-05-06T10:24:30.838219Z","shell.execute_reply":"2024-05-06T10:24:30.98989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reshape\n\nx_train=x_train.to_numpy()\ntest=test.to_numpy()\n\nx_train=x_train.reshape(-1,28,28,1)\n#(batch_size, height, width, channels)\n#The batch size affects how gradients are computed during backpropagation. It impacts the stability and speed of training.\ntest=test.reshape(-1,28,28,1)\n\nprint(\"x_train shape: \",x_train.shape)\nprint(\"test shape: \",test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:30.995225Z","iopub.execute_input":"2024-05-06T10:24:30.995709Z","iopub.status.idle":"2024-05-06T10:24:31.002504Z","shell.execute_reply.started":"2024-05-06T10:24:30.995669Z","shell.execute_reply":"2024-05-06T10:24:31.001418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#encoding\nimport keras\nfrom keras.utils import to_categorical  # convert to one-hot-encoding\ny_train = to_categorical(y_train, num_classes = 10)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:31.003563Z","iopub.execute_input":"2024-05-06T10:24:31.004654Z","iopub.status.idle":"2024-05-06T10:24:46.08133Z","shell.execute_reply.started":"2024-05-06T10:24:31.00462Z","shell.execute_reply":"2024-05-06T10:24:46.079719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test train split\nSince we have aready have split data, the reason we split up here is to use it for validating.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2) #splitting 80-20\n\nprint(\"x_train shape\",x_train.shape)\nprint(\"x_test shape\",x_val.shape)\nprint(\"y_train shape\",y_train.shape)\nprint(\"y_test shape\",y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:46.08318Z","iopub.execute_input":"2024-05-06T10:24:46.083876Z","iopub.status.idle":"2024-05-06T10:24:46.813984Z","shell.execute_reply.started":"2024-05-06T10:24:46.08384Z","shell.execute_reply":"2024-05-06T10:24:46.812487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and evaluation\n\n* Create models with layers\n* Compile model\n* Fit model\n* Evaluate the model","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:46.816089Z","iopub.execute_input":"2024-05-06T10:24:46.816573Z","iopub.status.idle":"2024-05-06T10:24:46.826485Z","shell.execute_reply.started":"2024-05-06T10:24:46.816532Z","shell.execute_reply":"2024-05-06T10:24:46.824929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating CNN model with layers","metadata":{}},{"cell_type":"code","source":"model=Sequential()\n\nmodel.add(Conv2D(32,(3,3),padding='Same',activation='relu',input_shape=(28,28,1)))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(64,(3,3),padding='Same',activation='relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\n#fully connected layer\nmodel.add(Flatten())\nmodel.add(Dense(128,'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10,'softmax'))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:46.828229Z","iopub.execute_input":"2024-05-06T10:24:46.828973Z","iopub.status.idle":"2024-05-06T10:24:47.013266Z","shell.execute_reply.started":"2024-05-06T10:24:46.828927Z","shell.execute_reply":"2024-05-06T10:24:47.012152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initializing the optimizer","metadata":{}},{"cell_type":"code","source":"optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:47.014701Z","iopub.execute_input":"2024-05-06T10:24:47.015046Z","iopub.status.idle":"2024-05-06T10:24:47.028526Z","shell.execute_reply.started":"2024-05-06T10:24:47.015017Z","shell.execute_reply":"2024-05-06T10:24:47.027163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**learning_rate** specifies the learning rate for the optimizer, **beta_1** and **beta_2** are the exponential decay rates for the moment estimates, and **epsilon** is a small value added to the denominator for numerical stability. \n\nThese values are the defaults used in TensorFlow's implementation of Adam, but you can adjust them based on your specific needs and the characteristics of your model and dataset.","metadata":{}},{"cell_type":"markdown","source":"### Compiling the model ","metadata":{}},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n\n#categorical_crossentropy--->It generalizes binary cross-entropy to multiple classes. \n#The model predicts class probabilities for each category, and the loss is minimized based on the true class probabilities 1.","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:47.029998Z","iopub.execute_input":"2024-05-06T10:24:47.030425Z","iopub.status.idle":"2024-05-06T10:24:47.041669Z","shell.execute_reply.started":"2024-05-06T10:24:47.030356Z","shell.execute_reply":"2024-05-06T10:24:47.040444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **metrics** argument expects a **list of strings**, even if you’re specifying only one metric. So, always use the list format to ensure proper functionality. ","metadata":{}},{"cell_type":"markdown","source":"Say you have a dataset of 10 examples (or samples). You have a batch size of 2, and you've specified you want the algorithm to run for 3 epochs. Therefore, in each epoch, you have 5 batches (10/2 = 5). Each batch gets passed through the algorithm, therefore you have 5 iterations per epoch.","metadata":{}},{"cell_type":"code","source":"epochs=10\nbatch_size=250","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:47.042974Z","iopub.execute_input":"2024-05-06T10:24:47.043433Z","iopub.status.idle":"2024-05-06T10:24:47.049341Z","shell.execute_reply.started":"2024-05-06T10:24:47.043404Z","shell.execute_reply":"2024-05-06T10:24:47.048208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augumentation\n\n Data augmentation supplements the creation of data variations that can help a model improve the accuracy of its predictions.\n \n* To avoid overfitting problem, we need to expand artificially our handwritten digit dataset\n* Alter the training data with small transformations to reproduce the variations of digit.\n* For example, the number is not centered The scale is not the same (some who write with big/small numbers) The image is rotated.","metadata":{}},{"cell_type":"code","source":"# data augmentation\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # dimesion reduction\n        rotation_range=5,  # randomly rotate images in the range 5 degrees\n        zoom_range = 0.1, # Randomly zoom image 10%\n        width_shift_range=0.1,  # randomly shift images horizontally 10%\n        height_shift_range=0.1,  # randomly shift images vertically 10%\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(x_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:47.051016Z","iopub.execute_input":"2024-05-06T10:24:47.051849Z","iopub.status.idle":"2024-05-06T10:24:47.150572Z","shell.execute_reply.started":"2024-05-06T10:24:47.051807Z","shell.execute_reply":"2024-05-06T10:24:47.149279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fitting the model\n\n","metadata":{}},{"cell_type":"code","source":"# Fit the model\n\ntrain_iterator = datagen.flow(x_train, y_train, batch_size=batch_size)\n\n\"\"\"This line creates an iterator for data augmentation using the ImageDataGenerator (datagen).\nIt generates augmented batches of training data.\nThe x_train and y_train are the input features (images) and corresponding labels (target values) for your training dataset.\nThe batch_size determines how many samples are processed at once in each batch.\"\"\"\n\n\n# Train your model\nhistory = model.fit(train_iterator,epochs = epochs, validation_data = (x_val,y_val), steps_per_epoch=len(x_train) // batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:24:47.152225Z","iopub.execute_input":"2024-05-06T10:24:47.152648Z","iopub.status.idle":"2024-05-06T10:27:03.610558Z","shell.execute_reply.started":"2024-05-06T10:24:47.15261Z","shell.execute_reply":"2024-05-06T10:27:03.609394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Converting the results saved as dataset\n\n\nThe **keras.callbacks.History** (which we saved here as **history**)object returned by the model.fit() method contains information about the training process, including metrics such as loss and accuracy recorded at each epoch. To access the contents inside the History object, you can use the following methods:\n\n**Accessing Training Metrics:**\nThe **History** object contains a dictionary called **history** that holds the training metrics.","metadata":{}},{"cell_type":"code","source":"history_df= pd.DataFrame(history.history)\nhistory_df","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:27:03.611732Z","iopub.execute_input":"2024-05-06T10:27:03.612634Z","iopub.status.idle":"2024-05-06T10:27:03.62813Z","shell.execute_reply.started":"2024-05-06T10:27:03.612591Z","shell.execute_reply":"2024-05-06T10:27:03.627014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting the **Training and Validation accuracy and loss**","metadata":{}},{"cell_type":"code","source":"\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_accuracy'].max()))\n\n\nplt.figure(figsize=(8, 6))\nplt.plot(history_df['loss'], label='Training Loss')\nplt.plot(history_df['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n\nplt.figure(figsize=(8, 6))\nplt.plot(history_df['accuracy'], label='Training Accuracy')\nplt.plot(history_df['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.show()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:27:03.630002Z","iopub.execute_input":"2024-05-06T10:27:03.63039Z","iopub.status.idle":"2024-05-06T10:27:04.233353Z","shell.execute_reply.started":"2024-05-06T10:27:03.630334Z","shell.execute_reply":"2024-05-06T10:27:04.232392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing with new samples","metadata":{}},{"cell_type":"code","source":"import cv2\n# Load the sample image\n\nfor i in range(1, 6):  # Assuming you have images named test1.jpg, test2.jpg, ..., test5.jpg\n    # Construct the filename for the current image\n    image_path = f'/kaggle/input/sample-test-set/test {i}.jpg'\n    test_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Replace 'sample_image.jpg' with your image file path\n\n\n    # Preview sample image\n    plt.imshow(test_image, cmap='gray')\n\n    # Format Image\n    img_resized = cv2.resize(test_image, (28, 28), interpolation=cv2.INTER_LINEAR)\n    img_resized = cv2.bitwise_not(img_resized)\n\n    # Preview reformatted image\n    plt.imshow(img_resized, cmap='gray')\n    plt.show()\n\n    # Prepare the image for prediction\n    input_image = img_resized.reshape(1, 28, 28, 1)  # Reshape to match the input format of your model\n    input_image = input_image.astype('float32') / 255  # Normalize the pixel values\n\n    prediction = model.predict(input_image)\n    predicted_digit = np.argmax(prediction)\n\n    print(\"Predicted Digit:\", predicted_digit)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T11:55:19.554122Z","iopub.execute_input":"2024-05-06T11:55:19.554895Z","iopub.status.idle":"2024-05-06T11:55:21.046119Z","shell.execute_reply.started":"2024-05-06T11:55:19.554844Z","shell.execute_reply":"2024-05-06T11:55:21.044854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Out of 5 test samples, it was able to identify only 2 correctly","metadata":{}},{"cell_type":"markdown","source":"### References\n\nhttps://www.kaggle.com/code/kanncaa1/convolutional-neural-network-cnn-tutorial/notebook","metadata":{}}]}